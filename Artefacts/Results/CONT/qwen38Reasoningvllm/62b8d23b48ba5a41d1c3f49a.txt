
Okay, I need to create a Python decorator called mru_cache that implements a Most Recently Used cache. Let me think about how to approach this.

First, the function signature is given with maxsize and typed parameters. The decorator should wrap a function, storing its results based on the arguments. The MRU algorithm means that when the cache is full, the least recently used item is evicted.

I remember that Python's functools.lru_cache is a similar decorator, but it uses LRU, not MRU. Wait, but maybe the user wants a custom implementation. However, the problem says to generate code based on the given function signature and docstring. So I need to implement the MRU cache from scratch.

Wait, but maybe the user is expecting a decorator that uses an MRU algorithm. Let me think about the data structures. For MRU, when a new item is added, it becomes the most recently used. When the cache is full, the least recently used is removed. So perhaps using a dictionary to store the cache and a list or another structure to track the order.

But implementing MRU can be done with a dictionary and a list. Each time a key is accessed, it's moved to the front of the list. When the cache is full, the last item in the list is removed.

Wait, but for efficiency, using a linked list might be better. However, in Python, for simplicity, maybe using an OrderedDict or a deque. Alternatively, using a dictionary and a list to track the order.

Wait, but the standard approach for MRU is to have a cache that keeps track of the order of access. When a key is accessed, it's moved to the front. When adding a new key, if the cache is full, the last item is removed.

So here's a plan:

The decorator will wrap the function. The cache will be a dictionary that maps arguments to results. Additionally, we need to track the order of usage. For MRU, the least recently used is the one that hasn't been used for the longest time. So when a new item is added, it's added to the front. When an item is accessed, it's moved to the front. When the cache is full, the last item is removed.

But how to track the order. One way is to use a list that keeps track of the order. Each time an item is accessed, it's moved to the front. When adding a new item, if the cache is full, remove the last item from the list.

But for efficiency, using a deque might be better. However, for the purposes of this problem, perhaps using a list and maintaining the order.

Alternatively, using an OrderedDict where the order is maintained. But OrderedDict's move_to_end method can be used. Wait, but OrderedDict's popitem(last=False) removes the first item, and popitem(last=True) removes the last. So for MRU, when adding a new item, if the cache is full, we remove the last item. So maybe using an OrderedDict and when adding a new item, if the size exceeds maxsize, we pop the last item.

Wait, but the standard LRU cache uses the OrderedDict's move_to_end and popitem(last=False) for LRU. For MRU, perhaps we need to move the accessed item to the front, and when adding a new item, if the cache is full, remove the last item.

So here's the plan:

The decorator will have a cache, which is an OrderedDict. When a function is called, the arguments are hashed (considering typed). The arguments are converted into a key. If the key is in the cache, we move it to the end (since it's now the most recently used). Wait, no. Wait, for MRU, when you access an item, it becomes the most recently used. So when you access it, you need to move it to the front. Wait, no. Wait, the order in which items are accessed determines their MRU status. So when you access an item, it's the most recently used. So when you add a new item, if the cache is full, you remove the least recently used, which would be the one that was added first.

Wait, maybe I'm getting confused between LRU and MRU. Let me think again.

In LRU, the least recently used is the one that hasn't been used for the longest time. So when you access an item, it's moved to the front. When the cache is full, the last item (the least recently used) is evicted.

In MRU, the most recently used is the one that was accessed last. So when you access an item, it's moved to the front. When the cache is full, the least recently used (the one that was added first) is evicted. Wait, but that's the same as LRU? Or is there a difference?

Wait, no. Wait, if you have a cache with items A, B, C. If you access A, it becomes the most recently used. So the order would be A, B, C. Then, if you access B, it's moved to the front: B, A, C. Then, if you access C, it's moved to front: C, B, A. So the least recently used is A. So when the cache is full, you evict A. That's the same as LRU. Wait, but maybe the MRU is different. Maybe in MRU, the least recently used is the one that was added first, and when you add a new item, you evict the least recently used. So in that case, the order is maintained such that the most recently used is at the front. So when adding a new item, if the cache is full, you remove the last item (the least recently used). So for example, if the cache has A, B, C (in order of insertion), then when you add D, you remove A. So the order becomes B, C, D. But when you access B, you move it to the front, making the order B, C, D. Then, when you access C, it's moved to front, making C, B, D. So the least recently used is D? Or is it B?

Hmm, perhaps I'm getting confused. Maybe the MRU cache is similar to LRU, but the eviction policy is different. Wait, perhaps the user intended for the cache to evict the least recently used item, which is the same as LRU. But the name is MRU. Maybe there's a misunderstanding here. Alternatively, maybe the user wants the cache to evict the most recently used item, which would be counterintuitive. But that's unlikely.

Alternatively, perhaps the MRU cache is a misnomer, and the user actually wants an LRU cache. However, the problem says to implement MRU. So perhaps I need to proceed with the assumption that when the cache is full, the least recently used item is evicted. Which is the same as LRU. But the name is MRU. Maybe it's a mistake. But given that the user provided the function signature and docstring, perhaps they want an MRU implementation, which may be different from LRU.

Alternatively, perhaps the MRU cache is implemented using a queue where the most recently used items are at the front. When a new item is added, it's placed at the front. When the cache is full, the item at the end is removed. So for example, if the cache is [A, B, C], and you add D, then the cache becomes [D, A, B], and C is removed. But when you access B, it's moved to the front: [D, A, B] becomes [B, D, A]. So the order is maintained such that the most recently used items are at the front. So when the cache is full, the least recently used is the one at the end.

So to implement this, we can use a list to track the order. Each time an item is accessed, it's moved to the front. When adding a new item, if the cache is full, remove the last item.

But how to efficiently do this. For example, when you access a key, you need to check if it's in the cache. If it is, move it to the front. If not, add it to the front. Then, if the size exceeds maxsize, remove the last item.

But for the cache, we need to track both the keys and their order. So perhaps using a dictionary to map keys to values, and a list to track the order. However, maintaining the list can be inefficient for large caches. But for the purposes of this problem, perhaps it's acceptable.

So here's the plan for the code:

The decorator will have a cache (a dictionary) and an order list. When a function is called, the arguments are converted into a key. If the key is in the cache, we move it to the front of the order list. Then, we return the cached value. If the key is not in the cache, we add it to the front of the order list. If the size exceeds maxsize, we remove the last item from the order list and the corresponding entry from the cache. Then, we store the result in the cache.

Wait, but how to handle the order list. For example, when a key is accessed, we need to check if it's in the cache. If it is, we remove it from the order list and add it to the front. Then, when adding a new key, we add it to the front. If the size exceeds maxsize, we remove the last item from the order list and the corresponding key from the cache.

But maintaining the order list can be done with a deque, but for simplicity, perhaps using a list and popping from the end.

But for efficiency, perhaps using a linked list structure. However, in Python, for simplicity, perhaps using a list and maintaining the order.

So, the code structure would be:

def mru_cache(maxsize=128, typed=False):
    def decorator(func):
        cache = {}
        order = []
        def wrapper(*args, **kwargs):
            # generate key
            key = (args, tuple(kwargs.items())) if typed else (args, kwargs)
            # check if key is in cache
            if key in cache:
                # move to front of order
                order.remove(key)
                order.insert(0, key)
                return cache[key]
            else:
                # check if cache is full
                if len(cache) >= maxsize:
                    # remove the last item from order and cache
                    last_key = order.pop()
                    del cache[last_key]
                # add new key to front of order
                order.insert(0, key)
                # store result in cache
                result = func(*args, **kwargs)
                cache[key] = result
                return result
        return wrapper
    return decorator

Wait, but this code has a problem. The order list is maintained as a list. When a key is accessed, we need to remove it from the order list and insert it at the front. However, the order list may have multiple entries, but in reality, each key is present once. So the remove(key) operation would work.

But for large order lists, this could be O(n) time. However, for the purposes of this problem, perhaps it's acceptable.

Another thing to consider is the typed parameter. If typed is True, then the arguments are considered different if their types differ. For example, 1 and 1.0 would be considered different keys. So, for the key, when typed is True, we need to convert the arguments into a tuple that includes the types. For example, for args, we can convert them into a tuple, and for kwargs, we can convert them into a sorted list of (key, value) pairs. Wait, but the original code in the example uses (args, tuple(kwargs.items())) if typed else (args, kwargs). Wait, but that would not be correct for the typed case. Because for the key, the arguments should be hashed, and for typed=True, the types of the arguments matter. So for example, if the function is called with (1, 2) and (1.0, 2.0), they would be considered different keys if typed is True.

But how to generate the key. For the args, we can convert them into a tuple. For the kwargs, we can convert them into a tuple of (key, value) pairs. However, since dictionaries are not hashable, we need to convert them into a tuple. But for the key, the order of the kwargs may matter. Wait, but in Python, the order of keyword arguments is preserved in Python 3.7+. However, for the purposes of the key, we need to ensure that the same arguments produce the same key. So, for the key, we can convert the args into a tuple, and the kwargs into a sorted list of (key, value) pairs. Wait, but that would make the order of the kwargs not matter. Wait, but for the key, the order of the keyword arguments should not matter. For example, if the function is called with a=1, b=2 or b=2, a=1, they should be considered the same key. So, to make the key consistent, we need to sort the kwargs by their keys. So, for the key, when typed is True, we can generate a tuple of (args, sorted(kwargs.items())). But if typed is False, we can generate a tuple of (args, frozenset(kwargs.items()))? Or perhaps, for the key, when typed is False, we can convert the arguments into a tuple, and the kwargs into a tuple of (key, value) pairs, but sorted by key. However, the original code in the example uses (args, tuple(kwargs.items())) if typed else (args, kwargs). Wait, but that would not be correct for the typed=False case. Because if typed is False, then the function's arguments are considered the same regardless of their types. For example, if the function is called with (1, 2) and (1.0, 2.0), they would be considered the same key if typed is False. But in the code above, the key would be (args, kwargs), which for the first case would be ((1, 2), {}) and for the second case ((1.0, 2.0), {}). So, if typed is False, the key would be based on the values, not their types. Wait, but the code in the example uses (args, tuple(kwargs.items())) if typed else (args, kwargs). Wait, that's not correct. Because when typed is False, the key should be based on the values, not their types. But the code as written would generate a key that includes the args and the kwargs. However, for the kwargs, if typed is False, the code would use the kwargs directly. But kwargs is a dictionary, which is not hashable. So, the code as written is incorrect.

Ah, right. The key must be hashable. So, for the key, we need to convert the args and kwargs into a hashable type. For args, we can convert them into a tuple. For kwargs, we can convert them into a tuple of (key, value) pairs. But the order of the kwargs may affect the key. So, to make the key consistent, we need to sort the kwargs by their keys. So, for example, if the function is called with a=1, b=2 or b=2, a=1, they should be considered the same key. So, for the key, when typed is False, we need to sort the kwargs by their keys. So, the key would be (args, sorted(kwargs.items())).

But how to handle the typed parameter. If typed is True, then the types of the arguments matter. So, for example, if the function is called with (1, 2) and (1.0, 2.0), they would be considered different keys. So, for the args, we can convert them into a tuple, but when typed is True, we need to include the types. Wait, but the args are already passed as arguments. For example, if the function is called with (1, 2), then args is (1, 2). If typed is True, then the key would include the types of the arguments. Wait, but how to get the types? Because the args are passed as positional arguments, and their types are determined at runtime. So, for the key, when typed is True, we need to include the types of the arguments. So, for the args, we can create a tuple of (type(arg), arg) for each arg. Similarly for the kwargs, we can create a tuple of (key, type(value), value) for each keyword argument.

Wait, but that would change the key based on the types of the arguments. So, for example, if the function is called with (1, 2) and (1.0, 2.0), the keys would be different. But how to generate such a key.

So, the key for the decorator should be generated as follows:

For the args, if typed is True, then each argument is converted to a tuple of (type(arg), arg). For example, for args (1, 2), the key part would be ((int, 1), (int, 2)). If typed is False, then the key part is (1, 2).

For the kwargs, if typed is True, then each keyword argument is converted to a tuple of (key, type(value), value). For example, for kwargs {'a': 1, 'b': 2}, the key part would be (('a', int, 1), ('b', int, 2)). If typed is False, then the key part is (('a', 1), ('b', 2)).

But this approach would make the key more complex. However, this is necessary to ensure that different types are considered different keys.

But how to implement this. Let's think.

For the args, we can create a tuple of (type(arg), arg) for each arg if typed is True. Otherwise, just the args as a tuple.

For the kwargs, we can create a tuple of (key, type(value), value) for each keyword argument if typed is True. Otherwise, a tuple of (key, value).

But in Python, tuples are hashable, so this should work.

So, the key generation code would be:

if typed:
    args_key = tuple((type(arg), arg) for arg in args)
    kwargs_key = tuple((k, type(v), v) for k, v in kwargs.items())
    key = (args_key, kwargs_key)
else:
    args_key = args
    kwargs_key = tuple(sorted(kwargs.items()))  # sorted to ensure order doesn't matter
    key = (args_key, kwargs_key)

Wait, but for the kwargs, when typed is False, we need to sort the items by key to ensure that the order of the keyword arguments doesn't affect the key. For example, if the function is called with a=1, b=2 or b=2, a=1, they should be considered the same key. So, the kwargs_key should be a tuple of sorted items.

So, the code for generating the key would be:

def generate_key(args, kwargs, typed):
    args_key = tuple((type(arg), arg) for arg in args) if typed else args
    kwargs_key = tuple(sorted((k, v) for k, v in kwargs.items())) if not typed else tuple((k, type(v), v) for k, v in kwargs.items())
    return (args_key, kwargs_key)

Wait, but for the typed=False case, the kwargs_key is a tuple of sorted (k, v) pairs. For typed=True, it's a tuple of (k, type(v), v) pairs. This way, the key includes the types when typed is True, and ignores them when typed is False.

But this is getting complicated. However, the original code in the example uses (args, tuple(kwargs.items())) if typed else (args, kwargs). But that's not correct because kwargs is a dictionary, which is not hashable. So, the code as written would not work. So, the correct approach is to convert the kwargs into a tuple of (key, value) pairs, sorted by key for typed=False.

So, the code for generating the key should be:

key = (args, tuple(sorted(kwargs.items()))) if not typed else (tuple((type(arg), arg) for arg in args), tuple((k, type(v), v) for k, v in kwargs.items()))

But this is getting complicated. However, the user's docstring says that the decorator saves up to maxsize results based on the MRU algorithm. So, the key generation is crucial for the correctness of the cache.

But perhaps the user expects that the key is generated as follows:

For typed=True, the key is (args, kwargs) where args and kwargs are converted into tuples that include their types. For example, args is converted into a tuple of (type(arg), arg), and kwargs is converted into a tuple of (key, type(value), value).

For typed=False, the key is (args, kwargs) where args is converted into a tuple of args, and kwargs is converted into a tuple of (key, value), sorted by key.

But how to implement this.

In code, the key generation would be:

if typed:
    args_key = tuple((type(arg), arg) for arg in args)
    kwargs_key = tuple((k, type(v), v) for k, v in kwargs.items())
else:
    args_key = args
    kwargs_key = tuple(sorted(kwargs.items()))

key = (args_key, kwargs_key)

But in Python, the args are already a tuple, so for typed=False, args_key is args. But for typed=True, we need to create a tuple of (type(arg), arg) for each arg.

So, the code for generating the key would be:

def generate_key(args, kwargs, typed):
    args_key = tuple((type(arg), arg) for arg in args) if typed else args
    kwargs_key = tuple((k, type(v), v) for k, v in kwargs.items()) if typed else tuple(sorted(kwargs.items()))
    return (args_key, kwargs_key)

But how to handle the case where the function is called with positional arguments and keyword arguments. For example, if the function is called as func(1, 2, a=3, b=4), then args is (1, 2), kwargs is {'a':3, 'b':4}.

So, the key is generated as described.

But for the code, the wrapper function needs to generate this key.

Putting it all together, the code for the decorator would be:

def mru_cache(maxsize=128, typed=False):
    def decorator(func):
        cache = {}
        order = []
        def wrapper(*args, **kwargs):
            # Generate key
            if typed:
                args_key = tuple((type(arg), arg) for arg in args)
                kwargs_key = tuple((k, type(v), v) for k, v in kwargs.items())
            else:
                args_key = args
                kwargs_key = tuple(sorted(kwargs.items()))
            key = (args_key, kwargs_key)
            # Check if key is in cache
            if key in cache:
                # Move to front of order
                order.remove(key)
                order.insert(0, key)
                return cache[key]
            else:
                # Check if cache is full
                if len(cache) >= maxsize:
                    # Remove the last item from order and cache
                    last_key = order.pop()
                    del cache[last_key]
                # Add new key to front of order
                order.insert(0, key)
                # Compute result
                result = func(*args, **kwargs)
                # Store in cache
                cache[key] = result
                return result
        return wrapper
    return decorator

But there's a problem with the order list. For example, when the key is in the cache, we remove it from the order list and insert it at the front. However, the order list may have multiple entries, but in reality, each key is present once. So the remove(key) operation would work.

But in Python, list.remove() removes the first occurrence of the element. However, since each key is unique in the order list, this should be okay.

Another issue is that for large caches, this approach may be inefficient because removing from the middle of a list is O(n) time. However, for the purposes of this problem, perhaps it's acceptable.

Testing this code, let's consider a simple function:

@mrucache(maxsize=2, typed=True)
def add(a, b):
    return a + b

add(1, 2)  # returns 3, cache has key ((int, 1), (int, 2)) -> 3
add(1, 2)  # returns 3, moves to front
add(3, 4)  # returns 7, cache now has ((int, 1), (int, 2)) and ((int, 3), (int, 4))
add(5, 6)  # cache is full, removes ((int, 1), (int, 2)), adds ((int, 5), (int, 6))

But when the cache is full, the last key in the order list is removed. So, the order list after adding (5,6) would be [((int,5), (int,6)), ((int,3), (int,4))].

So, this seems to work.

But what about when the same key is accessed again? It moves to the front.

Another test case: if the function is called with the same arguments, the key is found in the cache and moved to the front.

This should work.

However, the code may have some issues. For example, when the order list is empty, but that's handled by the code.

Another thing to consider is that the order list may have duplicate keys, but since the cache is a dictionary, each key is unique, so the order list should not have duplicates.

So, the code seems correct.

But I need to make sure that the key is generated correctly. For example, when typed is False, the kwargs are sorted. So, for the same function call with different keyword argument orders, they are considered the same key.

Yes, that's correct.

Now, the code for the decorator is written. However, the original code in the example uses the keyword 'typed' to determine whether to consider different types as different keys. The code above handles that.

So, the final code would be as written above.
