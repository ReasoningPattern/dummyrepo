Okay, I need to create an LFU cache decorator in Python. Hmm, the user provided the function signature and a docstring. Let me think about how to approach this.

First, I remember that LFU caching works by keeping track of how often each key is accessed. When the cache is full, the least frequently used items are evicted first. If there's a tie in frequency, maybe the least recently used among those is evicted.

The decorator needs to wrap a function, so the cache will store the results of the function calls. The parameters are maxsize and typed, which is similar to the @lru_cache in functools. The typed parameter allows different types of arguments to be treated as distinct, like 1 vs 1.0.

So, the structure would involve a dictionary to store the cached values. But each entry needs to track the frequency of access. Maybe each key in the cache will have an entry with the result, and a count of how many times it's been accessed. Also, to track the order for when frequencies are the same, perhaps a linked list or another structure. Wait, maybe using a collections.defaultdict for frequencies and a list or something else to track the order?

Alternatively, I can use a combination of a dictionary to hold the cache values, another to track the frequency counts, and perhaps a priority queue that uses both frequency and time. But that might get complicated.

Wait, another idea: Each time a key is accessed (either a hit or a new call), we increment its frequency. When the cache is full, we need to find the key with the smallest frequency. If there are multiple keys with the same minimum frequency, we might need to pick the oldest one among them.

So, how to structure this? Maybe the cache can have a dictionary for the actual stored values and their frequencies and maybe last access time. Let's think of each entry as a tuple: (value, frequency, timestamp). Then, when evicting, we look for the entry with the smallest frequency. If there are multiple, the one with the oldest timestamp.

But how to efficiently find the entry with the smallest frequency and timestamp? Using a priority queue where each element is (frequency, timestamp, key). The priority would be based first on frequency, then timestamp. But maintaining a priority queue might be tricky because when a key's frequency increases, we need to update its position in the queue. That's not straightforward with a heap, which is usually a min-heap but can't efficiently update elements.

Hmm, maybe another approach. Let's track for each key the frequency and the last access time. The cache can be a dictionary where each key has an entry with (value, freq, last_used). Additionally, we can have a frequency dictionary that maps frequency counts to a list of keys that have that frequency. But that might not directly help in finding the least frequently used.

Alternatively, perhaps we can keep all the keys in a structure that allows us to quickly find the one with the least frequency and least recent access. Maybe when adding a new entry or updating an existing one, we can track the frequencies and manage the cache accordingly.

Alternatively, since this is a decorator, maybe the cache can be managed with a dictionary that holds the key-value pairs, along with another dictionary that tracks the access counts and last access times. When the cache is full, we have to iterate through all the entries to find the one with the lowest frequency, and if there's a tie, the oldest one. But this could be O(n) for each insertion, which might be acceptable for small maxsizes but not optimal. However, given that the user is asking for a decorator, perhaps this is manageable.

Wait, but for a decorator, the maxsize is 128 by default, which is manageable. So even if on insertion we have to scan all entries to find the least frequently used, that might be okay for the given problem scope.

So here's a possible plan:

The decorator will have internal dictionaries:

- cache: key -> (value, freq, last_used_time)
- Or maybe just key -> (result, frequency, timestamp)

But how to track the frequencies? Each time a key is accessed (hit), its frequency is incremented, and timestamp is updated. For a miss, we add the key with frequency 1, and the current timestamp.

When the cache exceeds maxsize, we need to evict the key with the smallest frequency. If multiple keys have the same smallest frequency, we evict the one with the oldest timestamp.

So the steps would be:

1. When the function is called with arguments, we compute the key (using the same method as in lru_cache, considering the typed parameter).

2. Check if the key is in the cache:

   a. If yes: increment the frequency, update the timestamp, return the cached value.

   b. If no: compute the result, then check if the cache is at maxsize. If so, find the key to evict (the one with the smallest frequency, and if tie, oldest timestamp), remove it from the cache and the frequency structures. Then add the new key with frequency 1 and current timestamp.

But how to efficiently find the key with the minimal (frequency, timestamp)?

To do this, each time we add or update a key, we can track the current minimal frequency. But that might not be straightforward.

Alternatively, when the cache is full, we have to iterate through all the entries to find the one with the smallest frequency. If frequencies are the same, pick the one with the oldest timestamp.

This would be O(n) for each insertion when the cache is full, but for small maxsizes, this is acceptable.

So the code structure would involve:

- The decorator function, which returns a wrapper.

Inside the wrapper:

- Compute the key based on the arguments and the typed flag.

- Check if key is in the cache.

If hit:

- Increment the frequency, update the timestamp, return the value.

If miss:

- Compute the result by calling the original function.

- If the cache is at maxsize:

   - Iterate all keys in the cache to find the one with the least frequency. If tie, the oldest timestamp.

   - Remove that key from the cache.

- Add the new key with value, frequency 1, and current timestamp.

Now, how to track the timestamp? Maybe using a counter that increments each time a new entry is added or an existing one is accessed.

Wait, but the timestamp can be the time.time(), but that might have precision issues. Alternatively, a simple incrementing counter can work as a timestamp.

Let me think. Let's use a variable like _last_timestamp, which starts at 0 and increments by 1 each time an entry is accessed or added. So each access (hit or miss) increases the timestamp. That way, the timestamp can be used to break ties between entries with the same frequency.

So the steps would be:

Initialize:

cache = {}  # key: (value, frequency, timestamp)

_last_timestamp = 0

Then, when accessing a key:

If hit:

   frequency, old_ts = cache[key][1], cache[key][2]

   new_ts = _last_timestamp +1

   cache[key] = (value, frequency+1, new_ts)

   _last_timestamp = new_ts

Wait, actually, the timestamp is part of the stored data. So each time a key is accessed (hit or miss), we need to update its timestamp to the current _last_timestamp, then increment _last_timestamp.

Wait, perhaps:

Each time any operation happens (hit or miss), we increment the _last_timestamp. So the timestamp is a global counter.

So for a hit:

current_ts = _last_timestamp

cache[key] = (value, freq +1, current_ts)

then _last_timestamp +=1

Wait, perhaps:

When accessing a key (hit):

value, freq, old_ts = cache[key]

new_freq = freq +1

new_ts = _last_timestamp

cache[key] = (value, new_freq, new_ts)

_last_timestamp +=1

Similarly for a miss:

new_ts = _last_timestamp

then increment.

This way, every access (hit or miss) gets a new timestamp.

Okay, that seems manageable.

Now, putting this into code.

The decorator function:

def lfu_cache(maxsize=128, typed=False):

    # the wrapper function to be returned

    def decorating_function(user_function):

        # setup the cache structures here

        CACHE = {}

        LAST_TIMESTAMP = 0  # this is a mutable object, perhaps using a list?

        # Because in Python, variables in closures can't be reassigned unless nonlocal.

        # Alternatively, use an object to hold the timestamp.

        class TimestampHolder:
            def __init__(self):
                self.value = 0

        LAST_TIMESTAMP = TimestampHolder()

        # Also, need to track the number of entries in the cache.

        # Maybe just the length of CACHE?

        # Now, the wrapper function:

        def wrapper(*args, **kwds):

            nonlocal LAST_TIMESTAMP

            # compute the key, considering 'typed' parameter.

            # similar to the lru_cache's key calculation.

            # For typed, different types of arguments are treated as different keys.

            # So, for example, 1 vs 1.0 would be different if typed is True.

            key = args

            if kwds:

                key += (sorted(kwds.items()),)  # need a deterministic order for keyword args

            if typed:

                key += tuple(type(arg) for arg in args)

                key += tuple( (type(v) for (k,v) in sorted(kwds.items())) )

            # Wait, perhaps better to use a tuple of (type(arg), arg) for each arg.

            # Wait, the 'typed' parameter in functools.lru_cache's documentation says:

            # If typed is True, arguments of different types will be cached separately.

            # So, for example, 2 and 2.0 are different.

            # So, to create a key that includes the type of each argument.

            # So, for each positional argument, we can have (type(arg), arg), and similarly for keyword.

            # So, let's recalculate the key:

            # Need to create a tuple that includes types for each argument if typed is True.

            # Let's handle args first.

            key_parts = []

            for arg in args:

                if typed:

                    key_parts.append( (type(arg), arg) )

                else:

                    key_parts.append(arg)

            # Now handle the keyword arguments, sorted by key.

            for k in sorted(kwds):

                v = kwds[k]

                if typed:

                    key_parts.append( (type(k), k) )  # Wait, the key is the parameter's name, but types? Probably, no. Wait the key is the value of the parameter. Wait the 'typed' applies to the arguments' types, not the parameter's name.

                    key_parts.append( (type(v), v) )

                else:

                    key_parts.append( (k, v) )

            # Wait, maybe I'm overcomplicating. Let me think again.

            # The key must be hashable, so tuples of the arguments.

            # For each argument in args:

            # if typed is True, then the key includes the type of the argument.

            # So the key for an argument is (type(arg), arg).

            # Similarly for the keyword arguments.

            # So the full key would be a tuple of these.

            # Let me restructure this.

            # For args:

            arg_list = []

            for a in args:

                if typed:

                    arg_list.append( (type(a), a) )

                else:

                    arg_list.append(a)

            # For keyword args:

            kw_items = sorted(kwds.items())  # sorted by key name

            for (k, v) in kw_items:

                if typed:

                    # The key's type? Probably the key is a string, so its type is fixed.

                    # But the value's type is considered.

                    arg_list.append( ( (str, k), (type(v), v) ) )

                else:

                    arg_list.append( (k, v) )

            # Wait, perhaps better:

            # Each keyword arg is a tuple (k, v). So to include types:

            for (k, v) in sorted(kwds.items()):

                if typed:

                    key_part = ( (type(k), k), (type(v), v) )

                else:

                    key_part = (k, v)

                arg_list.append(key_part)

            # Then the full key is a tuple of all these parts.

            key = tuple(arg_list)

            # Now, check if this key is in the cache.

            current_ts = LAST_TIMESTAMP.value

            if key in CACHE:

                # It's a hit. Update frequency and timestamp.

                val, freq, old_ts = CACHE[key]

                new_freq = freq + 1

                new_ts = current_ts

                CACHE[key] = (val, new_freq, new_ts)

                # Increment the timestamp for next use.

                LAST_TIMESTAMP.value +=1

                return val

            else:

                # It's a miss. Need to compute the result.

                result = user_function(*args, **kwds)

                # Check if adding this will exceed maxsize.

                if len(CACHE) >= maxsize:

                    # Need to evict the least frequently used, then oldest.

                    # Iterate all keys to find the one with min (frequency, then timestamp).

                    min_freq = float('inf')

                    min_ts = float('inf')

                    evict_key = None

                    for k in CACHE:

                        val, freq, ts = CACHE[k]

                        if freq < min_freq or (freq == min_freq and ts < min_ts):

                            min_freq = freq

                            min_ts = ts

                            evict_key = k

                        elif freq == min_freq and ts == min_ts:

                            # same as current, so no change.

                            pass

                    # Remove the evict_key from the cache.

                    del CACHE[evict_key]

                # Now add the new entry.

                new_ts = current_ts

                CACHE[key] = (result, 1, new_ts)

                # Increment the timestamp.

                LAST_TIMESTAMP.value +=1

                return result

        return wrapper

    return decorating_function

Wait, but in the code above, when evicting, the code loops through all keys in the CACHE, and finds the one with the smallest frequency. If multiple have the same frequency, the one with the smallest timestamp (oldest) is chosen.

Wait, in the code, the condition is:

if freq < min_freq or (freq == min_freq and ts < min_ts):

Yes. Because when freq is equal, the one with smaller ts (earlier in time) is the older one. So that's correct.

But in the code, the initial min_ts is set to infinity, so for the first entry, it would set min_ts to ts of that key.

Hmm, this should work.

But there might be a problem with the key calculation. Let me check the key generation again.

The key for the args and kwds must be correctly generated. The code above is building a list of parts, then converting to a tuple. But for the keyword arguments, the code is adding each (k, v) as a tuple. However, in the case of typed=True, the key parts for the keywords would include the type of the key (which is a string, so type(str)), and the value's type. But maybe the key's type isn't necessary since the key is the parameter name, which is a string, and the 'typed' parameter applies to the arguments' values, not the parameter names. So perhaps the key's type isn't needed here, only the value's type.

Wait, the 'typed' parameter in functools.lru_cache's documentation says that if typed is True, arguments of different types are cached separately. So, for example, 1 (int) and 1.0 (float) would be different keys. The parameter names are not considered as their types, since they are strings. So for the keyword arguments, the key's name is part of the tuple, but its type isn't necessary. Only the value's type.

So in the code above, for the keyword arguments, when typed is True, the key part for the value would be (type(v), v), but the key (the parameter name) is just a string. So perhaps the code for keyword arguments should be:

for (k, v) in sorted(kwds.items()):

    if typed:

        # the key (parameter name) is a string, so its type is fixed. The value's type is important.

        key_part = ( (k, (type(v), v) ) )

    else:

        key_part = (k, v)

Wait, perhaps I'm overcomplicating. Let me think of the key for the keyword arguments. Each keyword argument is a pair (k, v). The key's type is not considered because it's the name (a string), and the 'typed' parameter applies to the values (v) of the arguments.

So for typed=True, the value's type is included. The key's name is part of the tuple, but its type (str) is not needed. So the key part for a keyword argument would be (k, (type(v), v)) if typed is True. Otherwise, (k, v).

So in the code:

for (k, v) in sorted(kwds.items()):

    if typed:

        # the key part is a tuple with the key (k) and the value's type and value.

        # Wait, but the key (k) is a string, so its type is fixed. So the key part is (k, (type(v), v))

        key_part = (k, (type(v), v))

    else:

        key_part = (k, v)

    arg_list.append(key_part)

Wait, but then the key for the keyword would be part of the tuple. So the full key for the function call would be a tuple of the positional arguments (each possibly including type and value), plus the sorted keyword arguments (each as (k, (type(v), v)) or (k, v)).

Hmm, but in the code I wrote earlier, for the positional arguments, when typed is True, each argument is represented as (type(a), a). So for a positional argument 2 (int) and 2.0 (float), they would be different. That's correct.

For the keyword arguments, the same applies. So the code should generate the key correctly.

Now, in the code above, the key is built correctly. Let's see.

Another possible issue is that the current_ts is set to LAST_TIMESTAMP.value, but when we do the evicting, the code is using the current_ts (the value before the increment). But when adding the new entry, we set new_ts to current_ts, then increment the timestamp. So the new entry's timestamp is current_ts (the value before any changes), and then after adding, the timestamp is increased.

Wait, in the hit case:

current_ts = LAST_TIMESTAMP.value

then, after setting the new entry's timestamp to current_ts, we do LAST_TIMESTAMP.value +=1.

In the miss case:

current_ts is the current value.

After evicting, we add the new entry with new_ts = current_ts, then increment the timestamp.

Yes, that's correct.

Now, possible edge cases:

What if the cache is exactly at maxsize, and we have to evict. The code loops through all keys to find the one with the smallest frequency and timestamp. That could be O(n) each time, but with maxsize=128, it's manageable.

Another possible issue is that when multiple keys have the same frequency and timestamp, but that's unlikely unless they were added in the same time step. But the timestamp is incremented each time, so each access has a unique timestamp. So in the case of two keys with the same frequency, their timestamps would differ, so the one with the older timestamp (smaller number) is chosen.

So the code should work.

Now, the code as written. Wait, in the code above, in the key building part, the code for the keyword arguments may have an error. Let me check again.

Wait, in the code I have written earlier, the arg_list for positional args is built first. Then for the keyword args:

for (k, v) in sorted(kwds.items()):

    if typed:

        key_part = ( (type(k), k), (type(v), v) )

    else:

        key_part = (k, v)

    arg_list.append(key_part)

Wait, that might be wrong. For the keyword arguments, the key (the parameter name) is a string, and its type is fixed (str). So in the 'typed' case, the value's type is important, but the key's type is not. So the key_part should be (k, (type(v), v)), not involving the type of k.

Ah, that's a mistake in the code. The parameter name's type (str) is not needed. So for the keyword's value, only the type of v matters. So the key_part should be (k, (type(v), v)) when typed is True, else (k, v).

So the code for the keyword arguments should be:

for (k, v) in sorted(kwds.items()):

    if typed:

        key_part = (k, (type(v), v))

    else:

        key_part = (k, v)

    arg_list.append(key_part)

That way, the key's name is included as a string, and the value's type and value are included if typed is True.

So that's a correction needed in the key generation code.

Another possible mistake is in the way the key is generated for the positional arguments. Let's see:

For the positional arguments:

for a in args:

    if typed:

        arg_list.append( (type(a), a) )

    else:

        arg_list.append(a)

Yes, that's correct. Each positional arg is either (type, value) or just the value.

Putting this all together, the code should work.

Now, in the code above, the key is built as a tuple of all these parts, which should be hashable. So the key can be stored in the dictionary.

Another thing to note is that the function must be hashable. So all the arguments must be hashable. The same as in lru_cache.

Now, the code for the LFU cache. The only thing left is the wrapper function and the code for evicting.

Wait, in the code I wrote earlier, in the evicting loop:

for k in CACHE:

    val, freq, ts = CACHE[k]

    if freq < min_freq or (freq == min_freq and ts < min_ts):

        min_freq = freq

        min_ts = ts

        evict_key = k

    elif freq == min_freq and ts == min_ts:

        # same as current, so no change.

        pass

But initially, min_freq is infinity, so the first entry will set min_freq to its frequency, min_ts to its ts, and evict_key to that key. Then, for each subsequent key, if its frequency is lower, or same frequency but lower ts, it will update.

This should work.

Now, the code as written. Let me try to code this correctly.

Wait, in the code above, the key generation for positional and keyword arguments may have an error. Let me correct that in the code.

The corrected key generation code:

Inside the wrapper function:

key_parts = []

for a in args:

    if typed:

        key_part = (type(a), a)

    else:

        key_part = a

    key_parts.append(key_part)

for (k, v) in sorted(kwds.items()):

    if typed:

        # the key part is (k, (type(v), v)), since the key name is a string, but its type isn't needed.

        key_part = (k, (type(v), v))

    else:

        key_part = (k, v)

    key_parts.append(key_part)

key = tuple(key_parts)

Yes, that's correct.

Now, putting all this into code.

Also, note that in the code, the CACHE is a dictionary inside the wrapper function. The LAST_TIMESTAMP is a TimestampHolder instance, initialized to 0.

Wait, in the code above, the LAST_TIMESTAMP is initialized as TimestampHolder(). So its value starts at 0.

When the first call comes in, current_ts is 0. After processing the hit or miss, the timestamp is incremented.

Another possible issue: when the cache is empty, and a new entry is added, the code should not evict, since the length was less than maxsize.

Yes.

Now, the code.

Wait, also, in the code above, when evicting, after finding the evict_key, the code does del CACHE[evict_key], then proceeds to add the new entry.

Then, the new entry's timestamp is current_ts (the value before the evicting loop's processing?), or after?

Wait, in the code:

When handling a miss:

current_ts = LAST_TIMESTAMP.value

if len(CACHE) >= maxsize:

    ... find evict_key and remove it.

Then:

new_ts = current_ts

CACHE[key] = (result, 1, new_ts)

LAST_TIMESTAMP.value +=1

Wait, the current_ts is the value before any changes. Then, after adding the new entry, the timestamp is incremented. So the new entry's timestamp is current_ts (the original value), and the next timestamp is current_ts +1.

Yes, that's correct.

Okay, I think this code should work. Let me write it properly now, with the corrected key parts.

Wait, here's the code:

Wait, in the code for the key parts, the keyword arguments are added as (k, (type(v), v)), so the tuple for each keyword is a pair (k, (type, value)), which is hashable.

Yes.

Now, the code:

def lfu_cache(maxsize=128, typed=False):
    def decorating_function(user_function):
        CACHE = {}
        class TimestampHolder:
            def __init__(self):
                self.value = 0
        last_timestamp = TimestampHolder()

        def wrapper(*args, **kwds):
            key_parts = []
            # Build the key
            for a in args:
                if typed:
                    key_parts.append( (type(a), a) )
                else:
                    key_parts.append(a)
            for k, v in sorted(kwds.items()):
                if typed:
                    key_parts.append( (k, (type(v), v)) )
                else:
                    key_parts.append( (k, v) )
            key = tuple(key_parts)

            current_ts = last_timestamp.value
            if key in CACHE:
                # Hit
                val, freq, ts = CACHE[key]
                new_freq = freq + 1
                new_ts = current_ts
                CACHE[key] = (val, new_freq, new_ts)
                last_timestamp.value += 1
                return val
            else:
                # Miss
                result = user_function(*args, **kwds)
                if len(CACHE) >= maxsize:
                    # Evict
                    min_freq = float('inf')
                    min_ts = float('inf')
                    evict_key = None
                    for k in CACHE:
                        val, freq, ts = CACHE[k]
                        if freq < min_freq or (freq == min_freq and ts < min_ts):
                            min_freq = freq
                            min_ts = ts
                            evict_key = k
                    del CACHE[evict_key]
                # Add new entry
                CACHE[key] = (result, 1, current_ts)
                last_timestamp.value += 1
                return result
        return wrapper
    return decorating_function

Wait, but in the code above, when evicting, the 'min_ts' is set to the smallest timestamp. So the code correctly picks the oldest entry when frequencies are equal.

Yes. Now, let's check the edge case where multiple entries have the same frequency and we need to pick the oldest.

Yes.

Another possible issue: when the cache is empty, adding the first entry. The code will not enter the evict loop, so the new entry is added correctly.

Another possible issue: when the cache is exactly at maxsize, but after evicting one entry, adding the new one brings it back to maxsize.

Yes.

I think this code should work. Let me see if there are any syntax errors.

Wait, in the wrapper function:

for k, v in sorted(kwds.items()):

Yes, sorted returns a list of tuples, so that's correct.

The key_parts are built correctly.

The key is a tuple of all the parts, so it's hashable.

The cache uses this key as the dictionary key.

Yes.

The timestamp is managed via the TimestampHolder's value.

The code increments the timestamp after each access (hit or miss).

Yes.

The eviction loop correctly finds the key to evict.

Yes.

Now, the code may have a problem if the user_function has keyword-only arguments, but since the code is using *args and **kwds, that's handled.

Another possible issue is that the code uses the key parts correctly. For example, if the function is called with different positional and keyword arguments, they are hashed correctly.

Yes.

So this should be the correct code.
